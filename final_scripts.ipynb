{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/eJO/4rdnxFfzEPB2/QA9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manuelapop/python_exercises/blob/main/final_scripts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "this is the semi gradient update exercise\n",
        "\n"
      ],
      "metadata": {
        "id": "k-fAvx2aEb93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question 1 chapter 9\n",
        "Semi-Gradient Update\n",
        "This problem presents a brief glimpse of the problems that can arise in off-policy learning with function approximation, through the concepts that have been introduced so far. If you would like a more detailed discussion on these issues, you may refer to Chapter 11. Let us now apply semi-gradient TD learning from Chapter 9 with batch updates (Section 6.3) to the following value-function approximation problem, based on a problem known as Baird's Counterexample:"
      ],
      "metadata": {
        "id": "4_TJlaZ008VF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "NUM_STATES = 6\n",
        "NUM_WEIGHTS = 7\n",
        "TOP_STATES = list(range(5))\n",
        "BOTTOM_STATE = 5\n",
        "TRANSITIONS = [(i, BOTTOM_STATE) for i in TOP_STATES] + [(BOTTOM_STATE, BOTTOM_STATE)]\n",
        "REWARD = 0.0\n",
        "\n",
        "def phi(state: int):\n",
        "    f = np.zeros(NUM_WEIGHTS)\n",
        "    if state in TOP_STATES:\n",
        "        f[0] = 1.0\n",
        "        f[state + 1] = 2.0\n",
        "    else:\n",
        "        f[0] = 2.0\n",
        "        f[6] = 1.0\n",
        "    return f\n",
        "\n",
        "def value(state, w):\n",
        "    return float(phi(state) @ w)\n",
        "\n",
        "def batch_td0_update(w, alpha, gamma):\n",
        "    dw = np.zeros_like(w)\n",
        "    for s, s_next in TRANSITIONS:\n",
        "        delta = REWARD + gamma * value(s_next, w) - value(s, w)\n",
        "        dw += alpha * delta * phi(s)\n",
        "    return w + dw\n",
        "\n",
        "def run_experiment(alpha, gamma, w0, num_batches):\n",
        "    w = np.array(w0, dtype=float)\n",
        "    print(\"Initial weights:\", w)\n",
        "\n",
        "    for b in range(1, num_batches + 1):\n",
        "        print(f\"\\n=== Batch {b} ===\")\n",
        "        print(f\"Bottom value before: {value(BOTTOM_STATE, w):.3f}\")\n",
        "        print(f\"Top value before:    {value(TOP_STATES[0], w):.3f}\")\n",
        "\n",
        "        w = batch_td0_update(w, alpha, gamma)\n",
        "\n",
        "        print(\"Updated weights:\", np.round(w, 3))\n",
        "        print(f\"Bottom value after:  {value(BOTTOM_STATE, w):.3f}\")\n",
        "        print(f\"Top value after:     {value(TOP_STATES[0], w):.3f}\")\n",
        "\n",
        "    print(\"\\nFinal weights:\", np.round(w, 3))\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--alpha\", type=float, default=0.1)\n",
        "    parser.add_argument(\"--gamma\", type=float, default=0.95)\n",
        "    parser.add_argument(\"--weights\", type=float, nargs=7,\n",
        "                        default=[1,1,1,1,1,1,5])\n",
        "    parser.add_argument(\"--batches\", type=int, default=1)\n",
        "\n",
        "    # Fix for Jupyter/Colab:\n",
        "    if \"ipykernel\" in sys.modules:\n",
        "        return parser.parse_args([])\n",
        "    else:\n",
        "        return parser.parse_args()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    run_experiment(args.alpha, args.gamma, args.weights, args.batches)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ49WZ2p_2i2",
        "outputId": "7b45f192-b231-4d59-e005-e2c16365982b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights: [1. 1. 1. 1. 1. 1. 5.]\n",
            "\n",
            "=== Batch 1 ===\n",
            "Bottom value before: 7.000\n",
            "Top value before:    3.000\n",
            "Updated weights: [2.755 1.73  1.73  1.73  1.73  1.73  4.965]\n",
            "Bottom value after:  10.475\n",
            "Top value after:     6.215\n",
            "\n",
            "Final weights: [2.755 1.73  1.73  1.73  1.73  1.73  4.965]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "question 2 chapter 10 Now we will calculate the values of State A and B for the MDP mentioned above. We put the problem description here again: 2 states and 3 states"
      ],
      "metadata": {
        "id": "mH85fV1C1AJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# ---------- Utility helpers ----------\n",
        "\n",
        "def make_periodic_reward(period):\n",
        "    \"\"\"\n",
        "    Given a list like [1, 0] or [1, 0, 0], return a function\n",
        "    reward(t) that returns the reward at time step t (0-based)\n",
        "    for that periodic sequence.\n",
        "    \"\"\"\n",
        "    T = len(period)\n",
        "    def reward_fn(t):\n",
        "        return period[t % T]\n",
        "    return reward_fn\n",
        "\n",
        "\n",
        "def average_reward(reward_fn, horizon=10_000):\n",
        "    \"\"\"\n",
        "    Time-average reward over a finite horizon, which approximates\n",
        "    the infinite-horizon average reward.\n",
        "    \"\"\"\n",
        "    total = 0.0\n",
        "    for t in range(horizon):\n",
        "        total += reward_fn(t)\n",
        "    return total / horizon\n",
        "\n",
        "\n",
        "def differential_value(reward_fn, avg_r, gamma=0.999, T=10_000):\n",
        "    \"\"\"\n",
        "    Approximate v(s) = sum_{t>=0} gamma^t (R_{t+1} - avg_r)\n",
        "    for a single starting state with reward sequence reward_fn(t).\n",
        "    \"\"\"\n",
        "    v = 0.0\n",
        "    g = 1.0\n",
        "    for t in range(T):\n",
        "        r = reward_fn(t)           # R_{t+1} (shifted, but deterministic)\n",
        "        v += g * (r - avg_r)\n",
        "        g *= gamma\n",
        "    return v\n",
        "\n",
        "\n",
        "# ---------- 2-state alternating example ----------\n",
        "\n",
        "def two_state_example():\n",
        "    print(\"=== 2-state alternating MDP ===\")\n",
        "    # State A: rewards 1,0,1,0,...\n",
        "    # State B: rewards 0,1,0,1,...\n",
        "    reward_A = make_periodic_reward([1, 0])\n",
        "    reward_B = make_periodic_reward([0, 1])\n",
        "\n",
        "    # Average reward should be 0.5\n",
        "    r_bar_A = average_reward(reward_A)\n",
        "    r_bar_B = average_reward(reward_B)\n",
        "    r_bar = 0.5 * (r_bar_A + r_bar_B)  # both equal, so any is fine\n",
        "\n",
        "    print(f\"Estimated average reward r(pi) ≈ {r_bar:.4f}\")\n",
        "\n",
        "    # Differential values for gamma close to 1\n",
        "    for gamma in [0.9, 0.99, 0.999, 0.9999]:\n",
        "        vA = differential_value(reward_A, r_bar, gamma=gamma)\n",
        "        vB = differential_value(reward_B, r_bar, gamma=gamma)\n",
        "        print(f\"gamma={gamma:>7}: v(A)≈{vA:.4f}, v(B)≈{vB:.4f}\")\n",
        "\n",
        "    print(\"Analytic values from theory: v(A)=+0.25, v(B)=-0.25\\n\")\n",
        "\n",
        "\n",
        "# ---------- 3-state ring example L'hopital rule ----------\n",
        "\n",
        "def three_state_example():\n",
        "    print(\"=== 3-state ring MDP (A -> B -> C -> A; reward 1 on A) ===\")\n",
        "\n",
        "    # Reward when ARRIVING in A is 1, else 0.\n",
        "    # We encode the *reward sequence* seen starting from each state.\n",
        "\n",
        "    # Start at A: A->B (0), B->C (0), C->A (1), then repeat: [0,0,1]\n",
        "    reward_A = make_periodic_reward([0, 0, 1])\n",
        "\n",
        "    # Start at B: B->C (0), C->A (1), A->B (0): [0,1,0]\n",
        "    reward_B = make_periodic_reward([0, 1, 0])\n",
        "\n",
        "    # Start at C: C->A (1), A->B (0), B->C (0): [1,0,0]\n",
        "    reward_C = make_periodic_reward([1, 0, 0])\n",
        "\n",
        "    # Average reward (should be 1/3)\n",
        "    # Any starting state gives the same long-run average, so we just use C\n",
        "    r_bar = average_reward(reward_C)\n",
        "    print(f\"Estimated average reward r(pi) ≈ {r_bar:.4f}  (theory: 1/3 ≈ 0.3333)\")\n",
        "\n",
        "    # Differential values for gamma close to 1\n",
        "    for gamma in [0.9, 0.99, 0.999, 0.9999]:\n",
        "        vA = differential_value(reward_A, r_bar, gamma=gamma)\n",
        "        vB = differential_value(reward_B, r_bar, gamma=gamma)\n",
        "        vC = differential_value(reward_C, r_bar, gamma=gamma)\n",
        "        print(\n",
        "            f\"gamma={gamma:>7}: \"\n",
        "            f\"v(A)≈{vA:.4f}, v(B)≈{vB:.4f}, v(C)≈{vC:.4f}\"\n",
        "        )\n",
        "\n",
        "    print(\"Analytic values from theory: v(A)=-1/3, v(B)=0, v(C)=+1/3\\n\")\n",
        "\n",
        "\n",
        "# ---------- Main ----------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    two_state_example()\n",
        "    three_state_example()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUL4DTbWF4LJ",
        "outputId": "ce5a1ced-c41e-41fd-bf4c-28594eac90bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 2-state alternating MDP ===\n",
            "Estimated average reward r(pi) ≈ 0.5000\n",
            "gamma=    0.9: v(A)≈0.2632, v(B)≈-0.2632\n",
            "gamma=   0.99: v(A)≈0.2513, v(B)≈-0.2513\n",
            "gamma=  0.999: v(A)≈0.2501, v(B)≈-0.2501\n",
            "gamma= 0.9999: v(A)≈0.1580, v(B)≈-0.1580\n",
            "Analytic values from theory: v(A)=+0.25, v(B)=-0.25\n",
            "\n",
            "=== 3-state ring MDP (A -> B -> C -> A; reward 1 on A) ===\n",
            "Estimated average reward r(pi) ≈ 0.3334  (theory: 1/3 ≈ 0.3333)\n",
            "gamma=    0.9: v(A)≈-0.3451, v(B)≈-0.0130, v(C)≈0.3560\n",
            "gamma=   0.99: v(A)≈-0.3411, v(B)≈-0.0078, v(C)≈0.3289\n",
            "gamma=  0.999: v(A)≈-0.4001, v(B)≈-0.0668, v(C)≈0.2669\n",
            "gamma= 0.9999: v(A)≈-0.7548, v(B)≈-0.5441, v(C)≈0.0346\n",
            "Analytic values from theory: v(A)=-1/3, v(B)=0, v(C)=+1/3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "question 3 chapter 12 Consider the following episode in an MRP:"
      ],
      "metadata": {
        "id": "QXz4nit11Di6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "def n_step_return(\n",
        "    rewards: List[float],\n",
        "    values: List[float],\n",
        "    gamma: float,\n",
        "    t: int,\n",
        "    n: int\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    chapter12\n",
        "    Compute the n-step return G_{t:t+n} for an episode.\n",
        "    Calculate the forward view-return for state 0, up to 6 decimal places.\n",
        "\n",
        "    rewards: [R1, R2, ..., RT]\n",
        "    values:  [V(S0), V(S1), ..., V(ST)]  # length = len(rewards) + 1\n",
        "    gamma: discount factor\n",
        "    t: starting time index (0-based, corresponds to S_t)\n",
        "    n: number of steps for the n-step return\n",
        "\n",
        "    Returns G_{t:t+n}.\n",
        "    \"\"\"\n",
        "    T = len(rewards)  # last reward index is T-1 (for R_T)\n",
        "\n",
        "    # Make sure n does not step beyond the episode\n",
        "    n = min(n, T - t)\n",
        "\n",
        "    G = 0.0\n",
        "    # First sum the discounted rewards\n",
        "    for k in range(1, n + 1):\n",
        "        G += (gamma ** (k - 1)) * rewards[t + k - 1]  # R_{t+k}\n",
        "\n",
        "    # Bootstrap with value at S_{t+n}, if it exists\n",
        "    if t + n <= T:\n",
        "        G += (gamma ** n) * values[t + n]\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "def full_return(\n",
        "    rewards: List[float],\n",
        "    gamma: float,\n",
        "    t: int\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Monte Carlo return G_t from time t to the end of the episode.\n",
        "    \"\"\"\n",
        "    G = 0.0\n",
        "    power = 0\n",
        "    for k in range(t, len(rewards)):\n",
        "        G += (gamma ** power) * rewards[k]\n",
        "        power += 1\n",
        "    return G\n",
        "\n",
        "\n",
        "def lambda_return(\n",
        "    rewards: List[float],\n",
        "    values: List[float],\n",
        "    gamma: float,\n",
        "    lam: float,\n",
        "    t: int\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Forward-view lambda-return G_t^λ for state S_t.\n",
        "\n",
        "    rewards: [R1, ..., RT]\n",
        "    values:  [V(S0), ..., V(ST)]\n",
        "    gamma: discount factor\n",
        "    lam: trace-decay parameter λ\n",
        "    t: starting time index (0-based)\n",
        "    \"\"\"\n",
        "    T = len(rewards)\n",
        "    max_n = T - t  # maximum steps from t until the end\n",
        "\n",
        "    # Last term: full return from t\n",
        "    G_MC = full_return(rewards, gamma, t)\n",
        "\n",
        "    # Sum of weighted n-step returns\n",
        "    weighted_sum = 0.0\n",
        "    for n in range(1, max_n):  # 1 ... (T - t - 1)\n",
        "        G_n = n_step_return(rewards, values, gamma, t, n)\n",
        "        weighted_sum += (lam ** (n - 1)) * G_n\n",
        "\n",
        "    if max_n > 0:\n",
        "        G_lambda = (1 - lam) * weighted_sum + (lam ** (max_n - 1)) * G_MC\n",
        "    else:\n",
        "        # No future rewards; λ-return is just V(S_t) by convention\n",
        "        G_lambda = values[t]\n",
        "\n",
        "    return G_lambda\n",
        "if __name__ == \"__main__\":\n",
        "    # From your screenshot example\n",
        "    rewards = [1, 0, 1, 0, 1]  # R1..R6\n",
        "    values  = [0.0, 0.1, 0.2, 0.3, 0.4]  # V(S0..S5)\n",
        "    gamma = 0.9\n",
        "    lam = 0.5\n",
        "    t = 0  # state S0\n",
        "\n",
        "    # Example: 1-step and 3-step returns from S0\n",
        "    print(\"G_{0:1} =\", n_step_return(rewards, values, gamma, t, 1))\n",
        "    print(\"G_{0:3} =\", n_step_return(rewards, values, gamma, t, 3))\n",
        "\n",
        "    # Lambda-return from S0\n",
        "    print(\"G_0^lambda =\", lambda_return(rewards, values, gamma, lam, t))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JQxi3K5Wg3_",
        "outputId": "92de6511-720e-4e60-d6ea-0d6fe74555a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G_{0:1} = 1.09\n",
            "G_{0:3} = 2.0287\n",
            "G_0^lambda = 1.37274625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "question 4 chapter 12 Now suppose we were computing the n-step truncated TD(\n",
        "), for horizon\n",
        "."
      ],
      "metadata": {
        "id": "PVLhtfId1Gdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "def n_step_return(rewards: List[float],\n",
        "                  values: List[float],\n",
        "                  gamma: float,\n",
        "                  t: int,\n",
        "                  n: int) -> float:\n",
        "    \"\"\"\n",
        "    G_{t:t+n}: n-step bootstrapped return starting at time t.\n",
        "    rewards: R_1, ..., R_T   (length T)\n",
        "    values : V(S_0), ..., V(S_T)  (state values)\n",
        "    \"\"\"\n",
        "    T = len(rewards)\n",
        "    n = min(n, T - t)          # cannot go past the end\n",
        "    G = 0.0\n",
        "    for k in range(1, n + 1):\n",
        "        G += (gamma ** (k - 1)) * rewards[t + k - 1]\n",
        "\n",
        "    # bootstrap if we haven't reached the end\n",
        "    if t + n < T:\n",
        "        G += (gamma ** n) * values[t + n]\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "def lambda_return_full(rewards: List[float],\n",
        "                       values: List[float],\n",
        "                       gamma: float,\n",
        "                       lam: float,\n",
        "                       t: int) -> float:\n",
        "    \"\"\"\n",
        "    Full forward-view lambda-return (no truncation).\n",
        "    \"\"\"\n",
        "    T = len(rewards)\n",
        "\n",
        "    # full Monte Carlo return G_t\n",
        "    G_full = 0.0\n",
        "    for k in range(1, T - t + 1):\n",
        "        G_full += (gamma ** (k - 1)) * rewards[t + k - 1]\n",
        "\n",
        "    s = 0.0\n",
        "    for n in range(1, T - t):\n",
        "        G_n = n_step_return(rewards, values, gamma, t, n)\n",
        "        s += (lam ** (n - 1)) * G_n\n",
        "\n",
        "    return (1 - lam) * s + (lam ** (T - t - 1)) * G_full\n",
        "\n",
        "\n",
        "def lambda_return_truncated(rewards: List[float],\n",
        "                            values: List[float],\n",
        "                            gamma: float,\n",
        "                            lam: float,\n",
        "                            t: int,\n",
        "                            h: int) -> float:\n",
        "    \"\"\"\n",
        "    n-step truncated TD(lambda) target with horizon h.\n",
        "    G_t^{lambda,h} = (1-lam) * sum_{n=1}^{h-1} lam^{n-1} G_{t:t+n}\n",
        "                     + lam^{h-1} * G_{t:t+h}\n",
        "    \"\"\"\n",
        "    T = len(rewards)\n",
        "    h = min(h, T - t)    # horizon can't exceed remaining length\n",
        "\n",
        "    s = 0.0\n",
        "    for n in range(1, h):\n",
        "        G_n = n_step_return(rewards, values, gamma, t, n)\n",
        "        s += (lam ** (n - 1)) * G_n\n",
        "\n",
        "    G_h = n_step_return(rewards, values, gamma, t, h)\n",
        "    return (1 - lam) * s + (lam ** (h - 1)) * G_h\n",
        "rewards = [1, 0, 1, 0, 1]        # R1..R5\n",
        "values  = [0, 0.1, 0.2, 0.3, 0.4]  # V(S0)..V(S4)\n",
        "gamma   = 0.9\n",
        "lam     = 0.5\n",
        "t       = 0\n",
        "h       = 3\n",
        "\n",
        "print(\"Truncated λ-return (h=3) from S0:\",\n",
        "      lambda_return_truncated(rewards, values, gamma, lam, t, h))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLU7jKBaYm1n",
        "outputId": "c9c0adec-c45c-4bd8-cfd8-6da8f304ca58"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Truncated λ-return (h=3) from S0: 1.342675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "question 5 chapter 12 We know that we can express the return\n",
        " in terms of the TD errors at steps\n",
        ",\n",
        "."
      ],
      "metadata": {
        "id": "bJHTEJHn1K9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "lambda_return_from_td_errors\n",
        "\n",
        "This function computes the forward-view λ-return G_t^λ for a *single* time step t\n",
        "using TD errors (δ_t, δ_{t+1}, …, δ_{T-1}).\n",
        "\n",
        "Formula:\n",
        "    G_t^λ = v_hat(S_t, w) + Σ_{k=t}^{T-1} (γλ)^{k-t} δ_k\n",
        "\n",
        "Inputs\n",
        "------\n",
        "td_errors : sequence of floats\n",
        "    TD errors from time t onward: [δ_t, δ_{t+1}, ..., δ_{T-1}]\n",
        "gamma : float\n",
        "    Discount factor γ (0 ≤ γ ≤ 1)\n",
        "lam : float\n",
        "    Trace decay parameter λ (0 ≤ λ ≤ 1)\n",
        "v_hat_st : float, optional (default=0.0)\n",
        "    Current value estimate for state S_t, v̂(S_t, w)\n",
        "\n",
        "Output\n",
        "------\n",
        "G_lambda : float\n",
        "    The λ-return G_t^λ for the starting time t corresponding to td_errors[0]\n",
        "\"\"\"\n",
        "\n",
        "def lambda_return_from_td_errors(td_errors, gamma, lam, v_hat_st=0.0):\n",
        "    \"\"\"Compute G_t^λ from TD errors starting at time t.\"\"\"\n",
        "    gl = v_hat_st\n",
        "    gl_mult = 1.0            # will hold (γλ)^{k-t}\n",
        "    gl_factor = gamma * lam  # γλ\n",
        "\n",
        "    for delta_k in td_errors:\n",
        "        gl += gl_mult * delta_k\n",
        "        gl_mult *= gl_factor\n",
        "\n",
        "    return gl\n",
        "\n",
        "\n",
        "# ------------------ Example from your screenshot ------------------ #\n",
        "if __name__ == \"__main__\":\n",
        "    td_errors = [1.1, 0.4, 0.8, -0.5, 0.7]  # δ_t ... δ_{t+4}\n",
        "    gamma = 0.9\n",
        "    lam = 0.5\n",
        "    v_hat_st = 0.0\n",
        "\n",
        "    G_lambda = lambda_return_from_td_errors(td_errors, gamma, lam, v_hat_st)\n",
        "    print(\"G_t^λ =\", G_lambda)      # -> 1.425141875...\n",
        "    print(\"Rounded to 6 decimals:\", round(G_lambda, 6))  # 1.425142\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFOUpZHOZsbt",
        "outputId": "78a3a348-b7ad-459e-898d-0cefd7067619"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G_t^λ = 1.4251418750000002\n",
            "Rounded to 6 decimals: 1.425142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: this is for the last question in the prcatice final examp and also question in the homework on eligibility traces chapter 12 Backward View"
      ],
      "metadata": {
        "id": "0XneJgF21U1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Backward-view eligibility traces and TD(λ) for tabular value functions.\n",
        "\n",
        "This script:\n",
        "\n",
        "  • Implements the backward-view eligibility trace update\n",
        "        z ← γ λ z + ∇v̂(S_t, w)\n",
        "    for a tabular / one-hot feature representation.\n",
        "\n",
        "  • Uses it to reproduce the example from the gridworld problem where the\n",
        "    eligibility trace after visiting states 5 → 4 → 3 has z[5] ≈ 0.0729.\n",
        "\n",
        "  • Implements tabular TD(λ) with backward-view traces and applies it to\n",
        "    the gridworld trajectory 5 → 4 → 3 → 2 → 1 → G with given rewards,\n",
        "    returning the final values V(1)…V(5).\n",
        "\"\"\"\n",
        "\n",
        "from typing import List\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1. Eligibility traces (backward view, tabular / one-hot features)\n",
        "# ---------------------------------------------------------------------\n",
        "def update_eligibility_trace(\n",
        "    trajectory: List[int],\n",
        "    num_states: int,\n",
        "    gamma: float,\n",
        "    lam: float,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Update eligibility traces for a given sequence of visited states.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "    trajectory : list[int]\n",
        "        Sequence of visited state indices in the order they are processed.\n",
        "        States are assumed to be integers in [0, num_states - 1].\n",
        "    num_states : int\n",
        "        Total number of distinct states (size of the tabular feature vector).\n",
        "    gamma : float\n",
        "        Discount factor γ.\n",
        "    lam : float\n",
        "        Trace decay parameter λ.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    z : np.ndarray, shape (num_states,)\n",
        "        Eligibility-trace vector after processing the whole trajectory.\n",
        "    \"\"\"\n",
        "    z = np.zeros(num_states, dtype=float)\n",
        "    gl = gamma * lam\n",
        "\n",
        "    for s in trajectory:\n",
        "        # Decay previous trace\n",
        "        z *= gl\n",
        "        # Add gradient for current state (one-hot feature for state s)\n",
        "        z[s] += 1.0\n",
        "\n",
        "    return z\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2. TD(λ) with backward-view eligibility traces (tabular case)\n",
        "# ---------------------------------------------------------------------\n",
        "def td_lambda_tabular(\n",
        "    states: List[int],\n",
        "    rewards: List[float],\n",
        "    num_states: int,\n",
        "    gamma: float,\n",
        "    lam: float,\n",
        "    alpha: float,\n",
        "    v_init: np.ndarray,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    One episode of tabular TD(λ) with backward-view eligibility traces.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "    states : list[int]\n",
        "        State sequence S_0, …, S_T (length T+1).\n",
        "    rewards : list[float]\n",
        "        Rewards r_{t+1} for transitions S_t → S_{t+1} (length T).\n",
        "    num_states : int\n",
        "        Number of distinct states.\n",
        "    gamma : float\n",
        "        Discount factor γ.\n",
        "    lam : float\n",
        "        Trace decay parameter λ.\n",
        "    alpha : float\n",
        "        Learning rate α.\n",
        "    v_init : np.ndarray\n",
        "        Initial value function, shape (num_states,).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    v : np.ndarray\n",
        "        Value function after processing the entire episode.\n",
        "    \"\"\"\n",
        "    v = v_init.copy()\n",
        "    z = np.zeros(num_states, dtype=float)\n",
        "\n",
        "    for t in range(len(rewards)):\n",
        "        s = states[t]\n",
        "        s_next = states[t + 1]\n",
        "        r = rewards[t]\n",
        "\n",
        "        # TD error δ_t = r_{t+1} + γ V(S_{t+1}) − V(S_t)\n",
        "        delta = r + gamma * v[s_next] - v[s]\n",
        "\n",
        "        # Eligibility trace update: z_t = γ λ z_{t−1} + x(S_t)\n",
        "        z *= gamma * lam\n",
        "        z[s] += 1.0\n",
        "\n",
        "        # Weight / value update\n",
        "        v += alpha * delta * z\n",
        "\n",
        "    return v\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3. Run both examples\n",
        "# ---------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # ===== Example 1: eligibility trace after states 5 → 4 → 3 =====\n",
        "    # We consider 6 states: [G, 1, 2, 3, 4, 5] with indices 0..5.\n",
        "    # Here we only care about the trace components.\n",
        "    trajectory = [5, 4, 3]  # indices of visited states\n",
        "\n",
        "    num_states = 6\n",
        "    gamma = 0.9\n",
        "    lam = 0.3\n",
        "\n",
        "    z = update_eligibility_trace(trajectory, num_states, gamma, lam)\n",
        "\n",
        "    print(\"=== Example 1: eligibility traces ===\")\n",
        "    print(\"Eligibility trace vector z after processing states 5 → 4 → 3:\")\n",
        "    print(z)\n",
        "    print(f\"Component of z associated with state 5 (index 5): {z[5]:.6f}\")\n",
        "    # Expected ≈ 0.0729\n",
        "\n",
        "    # ===== Example 2: TD(λ) values after full episode 5 → 4 → 3 → 2 → 1 → G =====\n",
        "    #\n",
        "    # State indices: 0 = G, 1 = state 1, …, 5 = state 5\n",
        "    # Episode: 5 → 4 → 3 → 2 → 1 → G\n",
        "    # Rewards for transitions:\n",
        "    #   5 → 4 :  0\n",
        "    #   4 → 3 : -0.1\n",
        "    #   3 → 2 : -0.1\n",
        "    #   2 → 1 :  0\n",
        "    #   1 → G : +1\n",
        "    states = [5, 4, 3, 2, 1, 0]\n",
        "    rewards = [0.0, -0.1, -0.1, 0.0, 1.0]\n",
        "\n",
        "    gamma = 0.9\n",
        "    lam = 0.5\n",
        "    alpha = 0.1\n",
        "\n",
        "    # Initial values:\n",
        "    # V(1) = 0, V(2) = 0, V(3) = -1, V(4) = 0, V(5) = 0, V(G) = 0\n",
        "    v_init = np.zeros(num_states, dtype=float)\n",
        "    v_init[3] = -1.0  # state 3\n",
        "\n",
        "    v_final = td_lambda_tabular(\n",
        "        states=states,\n",
        "        rewards=rewards,\n",
        "        num_states=num_states,\n",
        "        gamma=gamma,\n",
        "        lam=lam,\n",
        "        alpha=alpha,\n",
        "        v_init=v_init,\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Example 2: TD(λ) final values after full episode ===\")\n",
        "    for s in range(1, 6):\n",
        "        print(f\"V({s}) = {v_final[s]:.3f}\")\n",
        "    # To three decimals this should give approximately:\n",
        "    # V(1) ≈  0.100\n",
        "    # V(2) ≈  0.045\n",
        "    # V(3) ≈ -0.890\n",
        "    # V(4) ≈ -0.050\n",
        "    # V(5) ≈ -0.023\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQbu08p73kNn",
        "outputId": "c85e81bb-c3e5-4a2f-9f13-b1aea8d66fb2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Example 1: eligibility traces ===\n",
            "Eligibility trace vector z after processing states 5 → 4 → 3:\n",
            "[0.     0.     0.     1.     0.27   0.0729]\n",
            "Component of z associated with state 5 (index 5): 0.072900\n",
            "\n",
            "=== Example 2: TD(λ) final values after full episode ===\n",
            "V(1) = 0.100\n",
            "V(2) = 0.045\n",
            "V(3) = -0.890\n",
            "V(4) = -0.050\n",
            "V(5) = -0.023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7 chapter 12 calculating What is the TD-error\n",
        " for the transition from state 5 to state 4?"
      ],
      "metadata": {
        "id": "_IVEOk8T1ZDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "td_error.py\n",
        "\n",
        "Small helper script for Temporal-Difference (TD) learning.\n",
        "\n",
        "This file defines a function `td_error` that computes the TD error\n",
        "\n",
        "    δ_t = R_{t+1} + γ * V(S_{t+1}) - V(S_t)\n",
        "\n",
        "given:\n",
        "    - current state's value V(S_t)\n",
        "    - next state's value V(S_{t+1})\n",
        "    - reward R_{t+1} from the transition\n",
        "    - discount factor γ\n",
        "\n",
        "At the bottom there is a worked example using your numbers:\n",
        "transition from state 5 to state 4 with:\n",
        "    V(5) = 30, V(4) = 25, R = 0, γ = 0.9\n",
        "which should give δ = -7.5\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def td_error(v_current: float, v_next: float, reward: float, gamma: float) -> float:\n",
        "    \"\"\"\n",
        "    Compute the TD error for a single transition.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    v_current : float\n",
        "        V(S_t) - estimated value of the current state.\n",
        "    v_next : float\n",
        "        V(S_{t+1}) - estimated value of the next state.\n",
        "    reward : float\n",
        "        R_{t+1} - reward received when transitioning from S_t to S_{t+1}.\n",
        "    gamma : float\n",
        "        Discount factor γ (0 <= γ <= 1).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The TD error δ_t.\n",
        "    \"\"\"\n",
        "    return reward + gamma * v_next - v_current\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Example: transition from state 5 -> state 4 ---\n",
        "    V_5 = 30.0   # V(5)\n",
        "    V_4 = 25.0   # V(4)\n",
        "    R_5_to_4 = 0.0\n",
        "    gamma = 0.9\n",
        "\n",
        "    delta = td_error(V_5, V_4, R_5_to_4, gamma)\n",
        "    print(f\"TD error for transition 5 -> 4: δ = {delta}\")   # expected: -7.5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ICwKMAmbqvv",
        "outputId": "cd72a7da-d3e5-4671-bf09-857c226aa766"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TD error for transition 5 -> 4: δ = -7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8 chapter 12 What will be the new value of state\n",
        " (i.e.\n",
        ") if we make updates according to TD(\n",
        ") at each step with\n",
        ", at the end of the episode?"
      ],
      "metadata": {
        "id": "ws3JvmA81bDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TD(lambda) backward-view weight update for a tabular value function\n",
        "matching the homework example.\n",
        "\n",
        "- States: [goal, 1, 2, 3, 4, 5]\n",
        "- One-hot features (tabular)\n",
        "- γ = 0.9\n",
        "- λ = 0.3\n",
        "- α = 0.1\n",
        "- Trajectory: 5 → 4 → 3 → 2 → 1 → goal\n",
        "- Rewards: 0 on all steps, except last step reward = 1\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Discount, trace decay, learning rate\n",
        "gamma = 0.9\n",
        "lam = 0.3\n",
        "alpha = 0.1\n",
        "\n",
        "# Initial values (index = state number)\n",
        "# Index 0 = goal state\n",
        "w = np.array([0.0, 10.0, 15.0, 20.0, 25.0, 30.0], dtype=float)\n",
        "\n",
        "# Eligibility trace vector (same size as w)\n",
        "z = np.zeros_like(w)\n",
        "\n",
        "# Trajectory states (excluding the final terminal transition)\n",
        "trajectory = [5, 4, 3, 2, 1, 0]   # ending at goal (state 0)\n",
        "rewards = [0, 0, 0, 0, 1]        # reward for each transition\n",
        "\n",
        "print(\"Initial w =\", w)\n",
        "print(\"\\n===== TD(lambda) Updates =====\")\n",
        "\n",
        "for t in range(len(rewards)):\n",
        "    s = trajectory[t]       # current state\n",
        "    s_next = trajectory[t+1]  # next state\n",
        "    r = rewards[t]\n",
        "\n",
        "    # --- Eligibility trace update ---\n",
        "    # z = γλz + ∇v(s)\n",
        "    # For tabular: ∇v(s) is 1 for component s\n",
        "    z = gamma * lam * z\n",
        "    z[s] += 1.0\n",
        "\n",
        "    # --- TD error ---\n",
        "    delta = r + gamma * w[s_next] - w[s]\n",
        "\n",
        "    # --- Weight update ---\n",
        "    w = w + alpha * delta * z\n",
        "\n",
        "    print(f\"\\nState {s} → {s_next}\")\n",
        "    print(\"z after update:\", np.round(z, 6))\n",
        "    print(\"δ:\", round(delta, 6))\n",
        "    print(\"w after update:\", np.round(w, 6))\n",
        "\n",
        "print(\"\\n===== Final Value of State 5 =====\")\n",
        "print(\"V(5) =\", round(w[5], 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9AMVHc-cS97",
        "outputId": "ff2080f8-8544-443b-ebd3-682fec19e7ee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial w = [ 0. 10. 15. 20. 25. 30.]\n",
            "\n",
            "===== TD(lambda) Updates =====\n",
            "\n",
            "State 5 → 4\n",
            "z after update: [0. 0. 0. 0. 0. 1.]\n",
            "δ: -7.5\n",
            "w after update: [ 0.   10.   15.   20.   25.   29.25]\n",
            "\n",
            "State 4 → 3\n",
            "z after update: [0.   0.   0.   0.   1.   0.27]\n",
            "δ: -7.0\n",
            "w after update: [ 0.    10.    15.    20.    24.3   29.061]\n",
            "\n",
            "State 3 → 2\n",
            "z after update: [0.     0.     0.     1.     0.27   0.0729]\n",
            "δ: -6.5\n",
            "w after update: [ 0.       10.       15.       19.35     24.1245   29.013615]\n",
            "\n",
            "State 2 → 1\n",
            "z after update: [0.       0.       1.       0.27     0.0729   0.019683]\n",
            "δ: -6.0\n",
            "w after update: [ 0.       10.       14.4      19.188    24.08076  29.001805]\n",
            "\n",
            "State 1 → 0\n",
            "z after update: [0.       1.       0.27     0.0729   0.019683 0.005314]\n",
            "δ: -9.0\n",
            "w after update: [ 0.        9.1      14.157    19.12239  24.063045 28.997022]\n",
            "\n",
            "===== Final Value of State 5 =====\n",
            "V(5) = 28.997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9 chapter 12\n",
        "Just as the return can be written recursively in terms of the first reward and the return starting one-step later (i.e.\n",
        "), we can also get a recursive form for\n",
        " - return."
      ],
      "metadata": {
        "id": "ykuaAeR91dlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "lambda_return_recursive.py\n",
        "\n",
        "Compute the forward-view lambda-return G_t^λ using its recursive form:\n",
        "\n",
        "    G_t^λ = R_{t+1} + γ [ λ G_{t+1}^λ + (1 - λ) v_hat(S_{t+1}) ]\n",
        "\n",
        "This is the λ-return analogue of the recursive Bellman-style equation\n",
        "G_t = R_{t+1} + γ G_{t+1}, and is used in forward-view TD(λ) analysis.\n",
        "\"\"\"\n",
        "\n",
        "def lambda_return_recursive(R_tp1, gamma, lam, G_lambda_tp1, v_next):\n",
        "    \"\"\"\n",
        "    Compute the recursive λ-return G_t^λ.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    R_tp1 : float\n",
        "        Immediate reward R_{t+1}.\n",
        "    gamma : float\n",
        "        Discount factor γ.\n",
        "    lam : float\n",
        "        Trace-decay parameter λ.\n",
        "    G_lambda_tp1 : float\n",
        "        λ-return from the next time step, G_{t+1}^λ.\n",
        "    v_next : float\n",
        "        Bootstrapped value estimate at the next state, v_hat(S_{t+1}).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The λ-return G_t^λ.\n",
        "    \"\"\"\n",
        "    return R_tp1 + gamma * (lam * G_lambda_tp1 + (1 - lam) * v_next)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example with the values from the homework screenshot\n",
        "    R_tp1 = 10.0\n",
        "    gamma = 0.9\n",
        "    lam = 0.5\n",
        "    G_lambda_tp1 = 0.7\n",
        "    v_next = 9.0\n",
        "\n",
        "    G_lambda_t = lambda_return_recursive(R_tp1, gamma, lam, G_lambda_tp1, v_next)\n",
        "    print(f\"G_lambda_t = {G_lambda_t:.4f}\")  # -> 14.3650 (≈ 14.37)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he2xxekscyyX",
        "outputId": "f4bf7935-73f8-4e2f-87a1-6271f33f71aa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G_lambda_t = 14.3650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10 chapter 9 Now suppose we are using radial basis functions as our feature representation ( Use the equation on page 221 in Section 9.5.5 of the textbook. )."
      ],
      "metadata": {
        "id": "dhOA0ham1fPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def rbf(state, center, sigma=1.0):\n",
        "    \"\"\"\n",
        "    Radial Basis Function feature.\n",
        "    state  = (x, y)   current state coordinates\n",
        "    center = (cx, cy) center of this RBF\n",
        "    sigma  = width parameter (here sigma = 1.0)\n",
        "\n",
        "    rbf(s, c) = exp( - ||s - c||^2 / (2 * sigma^2) )\n",
        "    \"\"\"\n",
        "    x, y = state\n",
        "    cx, cy = center\n",
        "    dist_sq = (x - cx)**2 + (y - cy)**2\n",
        "    return math.exp(-dist_sq / (2.0 * sigma**2))\n",
        "\n",
        "\n",
        "def value_approx(state, centers, weight=0.2, sigma=1.0):\n",
        "    \"\"\"\n",
        "    Computes v(state) = sum_i w_i * rbf(state, c_i)\n",
        "    Here, all weights w_i are the same (= weight).\n",
        "    \"\"\"\n",
        "    total = 0.0\n",
        "    for c in centers:\n",
        "        total += rbf(state, c, sigma)\n",
        "    return weight * total\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Current state (yellow R)\n",
        "    state = (3, 2)\n",
        "\n",
        "    # Centers of the 5 RBFs (black dots)\n",
        "    centers = [\n",
        "        (1, 1),\n",
        "        (1, 2),\n",
        "        (2, 1),\n",
        "        (2, 3),\n",
        "        (3, 3),\n",
        "    ]\n",
        "\n",
        "    v = value_approx(state, centers, weight=0.2, sigma=1.0)\n",
        "    print(\"Value approximation at state (3,2):\", v)\n",
        "    # If you want it rounded:\n",
        "    print(\"Rounded to 2 decimals:\", round(v, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eEeJdL9-U2I",
        "outputId": "d7b5dbd6-0326-44cc-e35a-a3a140976844"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value approximation at state (3,2): 0.31194196478320596\n",
            "Rounded to 2 decimals: 0.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 11 Chapter 9 Now if we use a fourier basis as the feature representation, what would be the approximation of the state value at the current location?"
      ],
      "metadata": {
        "id": "UUHkN5np1g93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from itertools import product\n",
        "from typing import Sequence, List, Tuple\n",
        "\n",
        "\n",
        "def dot(a: Sequence[float], b: Sequence[float]) -> float:\n",
        "    \"\"\"Simple dot product between two vectors.\"\"\"\n",
        "    return sum(x * y for x, y in zip(a, b))\n",
        "\n",
        "\n",
        "def fourier_feature(state: Sequence[float], c: Sequence[float]) -> float:\n",
        "    r\"\"\"\n",
        "    Single Fourier-basis feature:\n",
        "        x_i(s) = cos(pi * s^T c_i)\n",
        "    where:\n",
        "        state s  = (x, y, ...)         current state (normalized)\n",
        "        c        = coefficient vector for this feature\n",
        "    \"\"\"\n",
        "    return math.cos(math.pi * dot(state, c))\n",
        "\n",
        "\n",
        "def fourier_value(\n",
        "    state: Sequence[float],\n",
        "    coeffs: Sequence[Sequence[float]],\n",
        "    weights: Sequence[float],\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Value approximation using a (possibly custom) set of Fourier-basis features.\n",
        "\n",
        "        v_hat(s) = sum_i w_i * cos(pi * s^T c_i)\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "    state   : current state vector\n",
        "    coeffs  : list of coefficient vectors c_i\n",
        "    weights : list of weights w_i (same length as coeffs)\n",
        "    \"\"\"\n",
        "    assert len(coeffs) == len(weights), \"coeffs and weights must have same length\"\n",
        "    total = 0.0\n",
        "    for c, w in zip(coeffs, weights):\n",
        "        total += w * fourier_feature(state, c)\n",
        "    return total\n",
        "\n",
        "\n",
        "def generate_fourier_coeffs(order: int, dim: int) -> List[Tuple[int, ...]]:\n",
        "    \"\"\"\n",
        "    Generate all Fourier coefficient vectors for a given max order.\n",
        "\n",
        "    For example, order=1, dim=2 gives:\n",
        "        (0,0), (0,1), (1,0), (1,1)\n",
        "\n",
        "    This is useful if you want an n-th order Fourier basis automatically.\n",
        "    \"\"\"\n",
        "    return list(product(range(order + 1), repeat=dim))\n",
        "if __name__ == \"__main__\":\n",
        "    # Current state (x, y)\n",
        "    state = (3.0, 2.0)\n",
        "\n",
        "    # Coefficient vectors c^0, c^1, c^2, c^3\n",
        "    coeffs = [\n",
        "        (0.0, 1.0),   # c^0\n",
        "        (1.0, 0.0),   # c^1\n",
        "        (0.2, 0.8),   # c^2\n",
        "        (0.8, 0.2),   # c^3\n",
        "    ]\n",
        "\n",
        "    # Weights w_0, ..., w_3\n",
        "    weights = [0.6, 0.3, 0.5, 0.3]\n",
        "\n",
        "    v = fourier_value(state, coeffs, weights)\n",
        "    print(\"Fourier value at state (3,2):\", v)\n",
        "    print(\"Rounded to 2 decimals:\", round(v, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_v9GXdKANiW",
        "outputId": "95401162-e965-446e-eb1b-eef50e068bc0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fourier value at state (3,2): 0.4618033988749894\n",
            "Rounded to 2 decimals: 0.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "question 12, chapter 2 gredy action"
      ],
      "metadata": {
        "id": "wcMbw8vLECJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_definite_steps(actions, rewards, k=None):\n",
        "    \"\"\"\n",
        "    Given sequences of actions and rewards for an epsilon-greedy bandit,\n",
        "    compute at which timesteps the epsilon case MUST have occurred.\n",
        "\n",
        "    We assume:\n",
        "      - sample-average action-value estimates\n",
        "      - initial Q_1(a) = 0 for all actions\n",
        "      - ties in Q are broken arbitrarily, so ANY action in the argmax(Q)\n",
        "        set could have been chosen greedily.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "    actions : list[int]\n",
        "        A_t sequence (1-based action indices).\n",
        "    rewards : list[float]\n",
        "        R_t sequence (same length as actions).\n",
        "    k : int or None\n",
        "        Number of arms. If None, uses max(actions).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    definite_eps_steps : list[int]\n",
        "        Timesteps t (1-based) at which the chosen action was NOT greedy,\n",
        "        so epsilon exploration definitely happened.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    assert len(actions) == len(rewards), \"actions and rewards must match in length\"\n",
        "\n",
        "    if k is None:\n",
        "        k = max(actions)\n",
        "\n",
        "    # Tracking statistics\n",
        "    Q = np.zeros(k)          # current Q-values\n",
        "    counts = np.zeros(k)     # how many times each action has been taken\n",
        "    sums = np.zeros(k)       # sum of rewards for each action\n",
        "\n",
        "    definite_eps_steps = []\n",
        "\n",
        "    print(\"=== Epsilon-definite analysis ===\\n\")\n",
        "    for t, (a, r) in enumerate(zip(actions, rewards), start=1):\n",
        "        a_idx = a - 1  # convert to 0-based index\n",
        "\n",
        "        # ---- BEFORE updating with (a, r), decide if this step MUST be epsilon ----\n",
        "        max_Q = np.max(Q)\n",
        "        greedy_actions = [i + 1 for i, q in enumerate(Q) if q == max_Q]\n",
        "\n",
        "        is_definitely_eps = (a not in greedy_actions)\n",
        "        if is_definitely_eps:\n",
        "            definite_eps_steps.append(t)\n",
        "\n",
        "        print(f\"Step {t}:\")\n",
        "        print(f\"  Q before step = {Q}\")\n",
        "        print(f\"  Greedy action(s) before step = {greedy_actions}\")\n",
        "        print(f\"  Chosen action = {a}\")\n",
        "        print(f\"  -> definitely epsilon? {is_definitely_eps}\\n\")\n",
        "\n",
        "        # ---- Now update Q with the observed reward ----\n",
        "        counts[a_idx] += 1\n",
        "        sums[a_idx] += r\n",
        "        Q[a_idx] = sums[a_idx] / counts[a_idx]\n",
        "\n",
        "    print(\"Final Q-values:\", Q)\n",
        "    print(\"Timesteps where epsilon definitely occurred:\", definite_eps_steps)\n",
        "    return definite_eps_steps\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Example from your question\n",
        "# ------------------------------------------------------------------\n",
        "actions = [1, 2, 2, 2, 3]\n",
        "rewards = [-1, 1, -2, 2, 0]\n",
        "\n",
        "epsilon_definite_steps(actions, rewards)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYR-KnspFUiZ",
        "outputId": "0dd745ad-828c-4e27-9074-52d66bf06026"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Epsilon-definite analysis ===\n",
            "\n",
            "Step 1:\n",
            "  Q before step = [0. 0. 0.]\n",
            "  Greedy action(s) before step = [1, 2, 3]\n",
            "  Chosen action = 1\n",
            "  -> definitely epsilon? False\n",
            "\n",
            "Step 2:\n",
            "  Q before step = [-1.  0.  0.]\n",
            "  Greedy action(s) before step = [2, 3]\n",
            "  Chosen action = 2\n",
            "  -> definitely epsilon? False\n",
            "\n",
            "Step 3:\n",
            "  Q before step = [-1.  1.  0.]\n",
            "  Greedy action(s) before step = [2]\n",
            "  Chosen action = 2\n",
            "  -> definitely epsilon? False\n",
            "\n",
            "Step 4:\n",
            "  Q before step = [-1.  -0.5  0. ]\n",
            "  Greedy action(s) before step = [3]\n",
            "  Chosen action = 2\n",
            "  -> definitely epsilon? True\n",
            "\n",
            "Step 5:\n",
            "  Q before step = [-1.          0.33333333  0.        ]\n",
            "  Greedy action(s) before step = [2]\n",
            "  Chosen action = 3\n",
            "  -> definitely epsilon? True\n",
            "\n",
            "Final Q-values: [-1.          0.33333333  0.        ]\n",
            "Timesteps where epsilon definitely occurred: [4, 5]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "question 13 - Chapter 3"
      ],
      "metadata": {
        "id": "3LiqzRrbXCob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_returns(gamma, rewards):\n",
        "    \"\"\"\n",
        "    Compute returns G_0, ..., G_n for a reward sequence with discount gamma.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "    gamma : float\n",
        "        Discount factor (0 <= gamma <= 1).\n",
        "    rewards : list[float]\n",
        "        Rewards [R1, R2, ..., Rn].\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    G : list[float]\n",
        "        Returns [G_0, G_1, ..., G_n], with G_n = 0\n",
        "        (assuming all rewards after R_n are zero).\n",
        "    \"\"\"\n",
        "\n",
        "    n = len(rewards)\n",
        "    # We store G_0 ... G_n (so length n+1)\n",
        "    G = [0.0] * (n + 1)\n",
        "\n",
        "    # Work backwards: G_n is 0 by assumption, then fill G_{n-1}, ..., G_0\n",
        "    for t in reversed(range(n)):\n",
        "        G[t] = rewards[t] + gamma * G[t + 1]\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Example from your screenshot\n",
        "# gamma = 0.5\n",
        "# R1 = -1, R2 = 2, R3 = 6, R4 = 3, R5 = 2\n",
        "# -------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    gamma = 0.5\n",
        "    rewards = [-1, 2, 6, 3, 2]\n",
        "\n",
        "    G = compute_returns(gamma, rewards)\n",
        "\n",
        "    # Pretty print: G_0 ... G_5\n",
        "    for t, g in enumerate(G):\n",
        "        print(f\"G_{t} = {g}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRENTNGWXKJA",
        "outputId": "304b34e3-7330-47d4-a2c7-45df220d21d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G_0 = 2.0\n",
            "G_1 = 6.0\n",
            "G_2 = 8.0\n",
            "G_3 = 4.0\n",
            "G_4 = 2.0\n",
            "G_5 = 0.0\n"
          ]
        }
      ]
    }
  ]
}